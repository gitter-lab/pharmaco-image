{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple CNN\n",
    "\n",
    "In this notebook, we will train a simple CNN (LeNet) end-to-end to predict one assay of compound activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sample Images\n",
    "\n",
    "For this model, we want to directly use 5-channel images. The images corresponding to one assay come from different plates (different files), so we want to have a nice function to extract those images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from torch.utils import data\n",
    "from collections import OrderedDict\n",
    "from glob import glob\n",
    "from os.path import join, exists, basename\n",
    "from json import load, dump\n",
    "from shutil import copyfile, rmtree\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Output Matrix\n",
    "\n",
    "In the output_matrix, we have `compound_broad_id`. We can use it to map to individual `pid` and `wid`.\n",
    "\n",
    "The structure is an array of `(pid, wid)` tuples corresponding to `compound_broad_id` in the output_matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = np.load('./resource/output_matrix_convert_collision.npz')\n",
    "output_matrix = output_data['output_matrix']\n",
    "compound_inchi = output_data['compound_inchi']\n",
    "compound_broad_id = output_data['compound_broad_id']\n",
    "assay = output_data['assay']\n",
    "cleaned_output_bids = [i[:13] for i in compound_broad_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27241, 212)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>wid</th>\n",
       "      <th>bid</th>\n",
       "      <th>cleaned_bid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25855</td>\n",
       "      <td>a01</td>\n",
       "      <td>BRD-K14087339-001-01-6</td>\n",
       "      <td>BRD-K14087339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25855</td>\n",
       "      <td>a02</td>\n",
       "      <td>BRD-K53903148-001-01-7</td>\n",
       "      <td>BRD-K53903148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25855</td>\n",
       "      <td>a03</td>\n",
       "      <td>BRD-K37357048-001-01-8</td>\n",
       "      <td>BRD-K37357048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25855</td>\n",
       "      <td>a04</td>\n",
       "      <td>BRD-K25385069-001-01-7</td>\n",
       "      <td>BRD-K25385069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25855</td>\n",
       "      <td>a05</td>\n",
       "      <td>BRD-K63140065-001-01-3</td>\n",
       "      <td>BRD-K63140065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pid  wid                     bid    cleaned_bid\n",
       "0  25855  a01  BRD-K14087339-001-01-6  BRD-K14087339\n",
       "1  25855  a02  BRD-K53903148-001-01-7  BRD-K53903148\n",
       "2  25855  a03  BRD-K37357048-001-01-8  BRD-K37357048\n",
       "3  25855  a04  BRD-K25385069-001-01-7  BRD-K25385069\n",
       "4  25855  a05  BRD-K63140065-001-01-3  BRD-K63140065"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./resource/merged_meta_table_406.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30413\n",
      "26939\n"
     ]
    }
   ],
   "source": [
    "print(len(set(cleaned_table_bids)))\n",
    "print(len(set(cleaned_output_bids).intersection(set(df['cleaned_bid']))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among 30413 imaged compounds, there are 26939 overlapping compounds in our 212 assays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dictionary cleaned_bid => [(pid, wid)]\n",
    "\n",
    "meta_bid_maps = {}\n",
    "for i, r in df.iterrows():\n",
    "    cur_bid = r['cleaned_bid']\n",
    "    cur_pid = r['pid']\n",
    "    cur_wid = r['wid']\n",
    "    \n",
    "    if cur_bid in meta_bid_maps:\n",
    "        meta_bid_maps[cur_bid].append((cur_pid, cur_wid))\n",
    "    else:\n",
    "        meta_bid_maps[cur_bid] = [(cur_pid, cur_wid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid_wids = [[] for i in range(output_matrix.shape[0])]\n",
    "\n",
    "# Iterate through cmpounds in the output matrix\n",
    "for i in range(output_matrix.shape[0]):\n",
    "    cur_bid = cleaned_output_bids[i]\n",
    "    pid_wids[i] = meta_bid_maps[cur_bid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite the old output matrix, so we don't need to extract pid_wids everytime\n",
    "np.savez('./resource/output_matrix_convert_collision_.npz',\n",
    "         output_matrix=output_matrix, compound_inchi=compound_inchi,\n",
    "         compound_broad_id=compound_broad_id, assay=assay,\n",
    "         cleaned_output_bids=cleaned_output_bids, pid_wids=pid_wids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Extract Images and Labels\n",
    "\n",
    "After getting the map from output compound to `(pid, wid)`, we can write a function to extract images and labels for one given assay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay = 192\n",
    "\n",
    "# Load the output matrix and each row's corresponding pid, wid\n",
    "output_data = np.load('./resource/output_matrix_convert_collision.npz')\n",
    "output_matrix = output_data['output_matrix']\n",
    "pid_wids = output_data['pid_wids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find selected compounds in this assay\n",
    "selected_index = output_matrix[:, assay] != -1\n",
    "selected_labels = output_matrix[:, assay][selected_index]\n",
    "selected_pid_wids = np.array(pid_wids)[selected_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the selected pid_wids and group them by pid\n",
    "# selected_wells has structure [(wid, pid, label)]\n",
    "selected_wells = []\n",
    "\n",
    "for i in range(len(selected_pid_wids)):\n",
    "    cur_pid_wids = selected_pid_wids[i]\n",
    "    cur_label = selected_labels[i]\n",
    "    \n",
    "    for pid_wid in cur_pid_wids:\n",
    "        selected_wells.append((pid_wid[0], pid_wid[1], int(cur_label)))\n",
    "\n",
    "# Group these wells by their pids\n",
    "selected_well_dict = {}\n",
    "for well in selected_wells:\n",
    "    cur_pid, cur_wid, cur_label = well[0], well[1], well[2]\n",
    "    \n",
    "    if cur_pid in selected_well_dict:\n",
    "        selected_well_dict[cur_pid].append((cur_wid, cur_label))\n",
    "    else:\n",
    "        selected_well_dict[cur_pid] = [(cur_wid, cur_label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_channels = ['ERSyto', 'ERSytoBleed', 'Hoechst', 'Mito', 'Ph_golgi']\n",
    "raw_paths = ['./{}-{}/*.tif'.format('{}', c) for c in raw_channels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_instance(pid, wid, label, output_dir='./output'):\n",
    "    \n",
    "    paths = [p.format(pid) for p in raw_paths]\n",
    "\n",
    "    # Dynamically count number of sids for this pid-wid\n",
    "    sid_files = [f for f in glob(paths[0]) if\n",
    "                        re.search(r'^.*_{}_s\\d_.*\\.tif$'.format(wid),\n",
    "                                  basename(f))]\n",
    "    sid_num = len(sid_files)\n",
    "\n",
    "    for sid in range(1, sid_num + 1):\n",
    "        # Each sid generates one instance\n",
    "        image_names, images = [], []\n",
    "\n",
    "        for p in paths:\n",
    "            # Search current pid-wid-sid\n",
    "            cur_file = [f for f in glob(p) if\n",
    "                        re.search(r'^.*_{}_s{}_.*\\.tif$'.format(wid, sid),\n",
    "                                  basename(f))]\n",
    "\n",
    "            # We should only see one result returned from the filter\n",
    "            if len(cur_file) != 1:\n",
    "                error = \"Found more than one file for {}-{}-{}.\".format(\n",
    "                    pid, wid, sid\n",
    "                )\n",
    "                raise ValueError(error)\n",
    "\n",
    "            image_names.append(cur_file[0])\n",
    "\n",
    "        # Read 5 channels\n",
    "        for n in image_names:\n",
    "            images.append(cv2.imread(n, -1) * 16)\n",
    "\n",
    "        # Store each image as a 5 channel 3d matrix\n",
    "        image_instance = np.array(images)\n",
    "\n",
    "        # Save the instance with its label\n",
    "        np.savez(join(output_dir, 'img_{}_{}_{}_{}.npz'.format(\n",
    "            pid, wid, sid, label\n",
    "        )), img=image_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid = 24277\n",
    "\n",
    "output_dir = './temp_1'\n",
    "\n",
    "for wid_tuple in selected_well_dict[pid]:\n",
    "    extract_instance(pid, wid_tuple[0], wid_tuple[1], output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 520, 696)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_plate(pid, selected_well_dict, output_dir='./output'):\n",
    "    \n",
    "    # Copy 5 zip files from gluster to the current directory\n",
    "    for c in raw_channels:\n",
    "        copyfile(\"/mnt/gluster/zwang688/{}-{}.zip\".format(pid, c),\n",
    "                 \"./{}-{}.zip\".format(pid, c))\n",
    "\n",
    "        # Extract the zip file and remove it\n",
    "        with zipfile.ZipFile(\"./{}-{}.zip\".format(pid, c), 'r') as fp:\n",
    "            fp.extractall('./')\n",
    "\n",
    "        os.remove(\"./{}-{}.zip\".format(pid, c))\n",
    "        \n",
    "    # Extract all instances from all selected wells in this plate\n",
    "    for wid_tuple in selected_well_dict[pid]:\n",
    "        extract_instance(pid, wid_tuple[0], wid_tuple[1], output_dir)\n",
    "        \n",
    "    # Clean up directories\n",
    "    for c in raw_channels:\n",
    "        rmtree(\"./{}-{}\".format(pid, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LeNet\n",
    "\n",
    "After having a nice function to extract 5-channel images from one given assay, we can start to implement LeNet using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. DataLoader\n",
    "\n",
    "Since we have a lot images, we don't want to load all of them into memory. Similarly to the `DataGenerator` in Keras, torch supports a runtime data loading mechanism. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    Define a dataset class so we can load data in the runtime.\n",
    "    Trainning dataset, vali dataset and test dataset can all use this class.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, img_names):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_names([string]): a list of image names in this dataset. The\n",
    "                name should be a relative path to a single image.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.img_names = img_names\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Tell pytorch how many instances are in this dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        return len(self.img_names)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Generate one image instance based on the given index.\n",
    "        \n",
    "        Args:\n",
    "            index(int): the index of the current item\n",
    "        \n",
    "        Return:\n",
    "            x(tensor): 5-channel 3d tensor encoding one cell image\n",
    "            y(int): 0 - negative assay, 1 - positive assay\n",
    "        \"\"\"\n",
    "        \n",
    "        # Read the image matrix and convert to torch tensor\n",
    "        cur_img_name = self.img_names[index]\n",
    "        mat = np.load(cur_img_name)['img'].astype(dtype=np.float32)\n",
    "        x = torch.from_numpy(mat)\n",
    "        \n",
    "        # Get the image label from its filename\n",
    "        y = int(re.sub(r'img_\\d+_.+_\\d_(\\d)\\.npz', r'\\1',\n",
    "                       basename(cur_img_name)))\n",
    "        \n",
    "        return x, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'batch_size': 32,\n",
    "    'shuffle': True,\n",
    "    'num_workers': os.cpu_count()\n",
    "}\n",
    "\n",
    "training_dataset = Dataset(glob('./temp_2/*.npz'))\n",
    "training_generator = data.DataLoader(training_dataset, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 5, 696, 696]) torch.Size([32])\n",
      "torch.Size([32, 5, 696, 696]) torch.Size([32])\n",
      "torch.Size([8, 5, 696, 696]) torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "for local_batch, local_labels in training_generator:\n",
    "    print(local_batch.shape, local_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. LeNet\n",
    "\n",
    "In this section, we use torch to implement a modified LeNet architecture which supports 5-channel inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/OZKLCxm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Modified LeNet architecture.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Create layers for the LeNet network.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(LeNet, self).__init__()\n",
    "        \n",
    "        # C1: 5 channel -> 6 filters (5x5)\n",
    "        self.conv1 = nn.Conv2d(5, 6, 5)\n",
    "        # C2: 6 filters -> 16 filters (5x5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # FC1: CP2 -> 120\n",
    "        self.fc1 = nn.Linear(171*171*16, 120)\n",
    "        # FC2: FC1 -> 84\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        # Output: FC2 -> 2 (activated or not)\n",
    "        self.output = nn.Linear(84, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Pytorch forward() method for autogradient.\n",
    "        \"\"\"\n",
    "        \n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        \n",
    "        # Flatten this layer to connect to FC lyaers\n",
    "        # size(0) is the batch size\n",
    "        out = out.view(out.size(0), -1)\n",
    "        \n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        # The original lenet5 doesnt use softmax.\n",
    "        out = F.softmax(self.output(out), dim=1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, device, training_generator, vali_generator,\n",
    "                    optimizer, epoch, early_stopping=None):\n",
    "    \n",
    "    # Set lenet to training mode\n",
    "    model.train()\n",
    "    \n",
    "    train_losses, y_predict_prob, y_true = [], [], []\n",
    "    for i, (cur_batch, cur_labels) in enumerate(training_generator):\n",
    "        \n",
    "        # Transfer tensor to GPU if available\n",
    "        cur_batch, cur_labels = cur_batch.to(device), cur_labels.to(device)\n",
    "\n",
    "        # Clean the gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Run the network forward\n",
    "        output = model(cur_batch)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(output, cur_labels)\n",
    "        train_losses.append(loss.detach().item())\n",
    "        y_predict_prob.extend(output.detach().numpy())\n",
    "        y_true.extend(cur_labels.numpy())\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Epoch {} - batch {}: loss = {}\".format(epoch, i, loss))\n",
    "\n",
    "        # Backpropogation and update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Convert tensor to numpy array so we can use sklearn's metrics\n",
    "    y_predict_prob = np.stack(y_predict_prob)\n",
    "    y_predict = np.argmax(y_predict_prob, axis=1)\n",
    "    y_true = np.array(y_true)\n",
    "    \n",
    "    # Average losses over different batches. Each loss corresponds to the mean\n",
    "    # loss within that batch (reduction=\"mean\").\n",
    "    train_loss = np.mean(train_losses)\n",
    "    train_acc = metrics.accuracy_score(y_true, y_predict)\n",
    "        \n",
    "    # After training for this epoch, we evaluate this current model on the\n",
    "    # validation set\n",
    "    model.eval()\n",
    "    vali_losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for cur_batch, cur_labels in vali_generator:\n",
    "            cur_batch, cur_labels = cur_batch.to(device), cur_labels.to(device)\n",
    "            output = model(cur_batch)\n",
    "            \n",
    "            loss = criterion(output, cur_labels)\n",
    "            vali_losses.append(loss.detach().item())\n",
    "    \n",
    "    # Average losses over different batches. Each loss corresponds to the mean\n",
    "    # loss within that batch (reduction=\"mean\").\n",
    "    vali_loss = np.mean(vali_losses)\n",
    "\n",
    "    # Early stopping (the real stopping is outside of this function)\n",
    "    if early_stopping:\n",
    "        if vali_loss < early_stopping['best_loss']:\n",
    "            early_stopping['best_loss'] = vali_loss\n",
    "            early_stopping['wait'] = 0\n",
    "        else:\n",
    "            early_stopping['wait'] += 1\n",
    "            \n",
    "    return train_loss, train_acc, vali_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_generator):\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    test_losses, y_predict_prob, y_true = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for cur_batch, cur_labels in test_generator:\n",
    "            # Even there is only forward() in testing phase, it is still faster\n",
    "            # to do it on GPU\n",
    "            cur_batch, cur_labels = cur_batch.to(device), cur_labels.to(device)\n",
    "            \n",
    "            output = model(cur_batch)\n",
    "            loss = criterion(output, cur_labels)\n",
    "            \n",
    "            # Track the loss and prediction for each batch\n",
    "            test_losses.append(loss.detach().item())\n",
    "            y_predict_prob.extend(output.detach().numpy())\n",
    "            y_true.extend(cur_labels.numpy())\n",
    "\n",
    "    # Convert tensor to numpy array so we can use sklearn's metrics\n",
    "    # sklearn loves 1d proba array of the activated class\n",
    "    y_predict_prob = np.stack(y_predict_prob)[:, 1]\n",
    "    y_predict = [1 if i >= 0.5 else 0 for i in y_predict_prob]\n",
    "    y_true = np.array(y_true)\n",
    "    \n",
    "    # Take the average of batch loss means\n",
    "    test_loss = np.mean(test_losses)\n",
    "    test_acc = metrics.accuracy_score(y_true, y_predict)\n",
    "    \n",
    "    print(\"Testing on {} instances, the accuracy is {:.2f}.\".format(\n",
    "        len(test_generator), test_acc\n",
    "    ))\n",
    "    \n",
    "    return test_loss, test_acc, y_predict_prob, y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare for training a LeNet. We need to create data generators and an early stopping tracking dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6 training samples, 2 validation samples, and 2 test samples.\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'batch_size': 32,\n",
    "    'shuffle': True,\n",
    "    'num_workers': os.cpu_count()\n",
    "}\n",
    "\n",
    "img_names = glob('./temp_1/*.npz')\n",
    "\n",
    "# Randomly split img_names into three sets\n",
    "img_names = shuffle(img_names)\n",
    "quintile_len = len(img_names) // 5\n",
    "vali_names = img_names[: quintile_len]\n",
    "test_names = img_names[quintile_len: quintile_len * 2]\n",
    "train_names = img_names[quintile_len * 2: ]\n",
    "\n",
    "print(\"There are {} training samples, {} validation samples, and {} test samples.\".\\\n",
    "      format(len(train_names), len(vali_names), len(test_names)))\n",
    "\n",
    "# Create data generators\n",
    "training_dataset = Dataset(train_names)\n",
    "training_generator = data.DataLoader(training_dataset, **params)\n",
    "\n",
    "vali_dataset = Dataset(vali_names)\n",
    "vali_generator = data.DataLoader(vali_dataset, **params)\n",
    "\n",
    "test_dataset = Dataset(test_names)\n",
    "test_generator = data.DataLoader(test_dataset, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - batch 0: loss = 1.31326162815094\n"
     ]
    }
   ],
   "source": [
    "# Run on GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Use cross-entropy as our loss funciton\n",
    "lenet = LeNet()\n",
    "lenet.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = optim.Adam(lenet.parameters(), lr=0.001)\n",
    "\n",
    "# Init early stopping\n",
    "early_stopping_dict = {\n",
    "    'best_loss': np.inf,\n",
    "    'wait': 0,\n",
    "    'patience': 20\n",
    "}\n",
    "\n",
    "for e in range(1):\n",
    "    train_loss, train_acc, vali_loss = train_one_epoch(\n",
    "        lenet, device, training_generator, vali_generator,\n",
    "        optimizer, e, early_stopping=early_stopping_dict\n",
    "    )\n",
    "    \n",
    "    if early_stopping_dict['wait'] > early_stopping_dict['patience']:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 6, 692, 692]\n",
      "[2, 6, 346, 346]\n",
      "[2, 16, 342, 342]\n",
      "[2, 16, 171, 171]\n",
      "[2, 467856]\n",
      "[2, 2]\n",
      "Testing on 1 instances, the accuracy is 0.50.\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc, y_predict_prob, y_true = test(lenet, device, test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
